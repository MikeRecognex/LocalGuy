=---
title: "llama.cpp Adds Model Context Protocol Support for Testing"
date: 2026-02-11
description: "llama.cpp now includes MCP support after over a month of development, enabling system message injection, CORS proxy functionality, and server selection capabilities for local LLM deployments."
tags:
  - daily-digest
  - llama-cpp
  - mcp
  - agents
  - open-source
status: draft
---

The popular llama.cpp inference engine has received a major update with Model Context Protocol (MCP) support now ready for testing. After over a month of development work by contributor allozaur, the implementation includes several impressive features including system message injection, CORS proxy functionality on the llama-server backend, and comprehensive MCP server selection and management capabilities.

This development represents a significant step forward for local LLM deployments, as MCP enables seamless integration with external tools and services while maintaining the privacy and control benefits of local inference. The addition positions llama.cpp users to build more sophisticated agent-like applications that can interact with various data sources and APIs.

For practitioners running local LLMs, this update opens up new possibilities for creating powerful RAG pipelines and tool-using applications without relying on cloud-based services. You can follow the development progress and testing updates on the [original Reddit discussion](https://i.redd.it/yyar9f4hdqig1.png).

---
*Source: [r/LocalLLaMA](https://i.redd.it/yyar9f4hdqig1.png) Â· Relevance: 9/10*
    